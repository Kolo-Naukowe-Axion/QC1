{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jLzI6vhj-pd"
      },
      "source": [
        "# Quantum Classfier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LKSJGRyG0ouM",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "acab6faa-a2a0-4aec-9d88-31e6795d6811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: qiskit in /usr/local/lib/python3.12/dist-packages (1.4.5)\n",
            "Requirement already satisfied: qiskit_machine_learning in /usr/local/lib/python3.12/dist-packages (0.8.4)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.17.1)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from qiskit) (1.15.3)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit) (1.14.0)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit) (2.9.0.post0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit) (5.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit) (4.15.0)\n",
            "Requirement already satisfied: symengine<0.14,>=0.11 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.13.0)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.12/dist-packages (from qiskit_machine_learning) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=40.1 in /usr/local/lib/python3.12/dist-packages (from qiskit_machine_learning) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit_machine_learning) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit_machine_learning) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install qiskit qiskit_machine_learning\n",
        "!pip install ucimlrepo\n",
        "\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit import ParameterVector\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
        "import numpy as np\n",
        "# from qiskit.primitives import StatevectorEstimator\n",
        "from qiskit.primitives import Estimator\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "from qiskit_machine_learning.gradients import ParamShiftEstimatorGradient\n",
        "import sys          # Standard library module for system-specific parameters and functions\n",
        "import subprocess   # Standard library module for spawning new processes\n",
        "from sklearn.preprocessing import MinMaxScaler # Importuje MinMaxScaler do skalowania danych\n",
        "from sklearn.model_selection import train_test_split # Importuje train_test_split do podziału danych\n",
        "from ucimlrepo import fetch_ucirepo     # Importuje fetch_ucirepo do pobierania zestawów danych z UCI ML Repository\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enUz4SJl6Fnh"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "\n",
        "def ensure_package(pkg_name, import_name=None):\n",
        "    import_name = import_name or pkg_name\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "    except ImportError:\n",
        "        print(f'Installing {pkg_name}...') # Installing {pkg_name}...\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
        "        print(f'Installation of {pkg_name} finished') # Installation of {pkg_name} finished\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while checking package {pkg_name}: {e}\") # An unexpected error occurred while checking package {pkg_name}: {e}\n",
        "\n",
        "\n",
        "ensure_package('numpy')\n",
        "ensure_package('scikit-learn', 'sklearn')\n",
        "ensure_package('ucimlrepo')\n",
        "\n",
        "\n",
        "# Function for data preparation\n",
        "def prepare_data():\n",
        "\n",
        "    # Fetches the 'banknote authentication' dataset from the UCI ML Repository\n",
        "    banknote_authentication = fetch_ucirepo(id=267)\n",
        "    # Extracts features (independent variables) from the dataset\n",
        "    X = banknote_authentication.data.features\n",
        "    # Extracts the target variable (dependent variable) from the dataset\n",
        "    y = banknote_authentication.data.targets\n",
        "\n",
        "    X = X.to_numpy() # Converts features to a NumPy array\n",
        "    y = y.to_numpy().ravel() # Converts the target variable to a flattened NumPy array\n",
        "\n",
        "    # Splits the data into training and testing sets\n",
        "    # test_size=0.2 means 20% of the data for testing, random_state ensures split repeatability\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Data scaling\n",
        "    # Initializes MinMaxScaler, which scales features to the range from 0 to pi\n",
        "    scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
        "    # Fits the scaler to the training data and simultaneously scales it\n",
        "    # 'fit_transform' is used on the training data so the scaler learns min/max values\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    # Scales the test data using parameters learned from the training data\n",
        "    # 'transform' is used on the test data to prevent data leakage\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Returns the prepared and scaled data\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "# Executes the prepare_data() function only if the script is run directly\n",
        "if __name__ == \"__main__\":\n",
        "    # Calls the data preparation function and assigns the returned values to variables\n",
        "    X_tr, X_te, y_tr, y_te = prepare_data()\n",
        "    print(\"test\") # Prints \"test\" to the console, signaling the end of the operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVvwAGxS8oGq"
      },
      "source": [
        "Poniższy kod odpowiada za anstaz. Powstał on w oparciu o bibliteke Qiskit i korzysta z wbudowanych w nią funkcji, takich jak ParameterVektor służący do łatwego iterowania po parametrach bramek rotacyjnych. Sieć zakładay wykożystanie 4 qbitów oraz parzystej ilości warst. Każda warstwa składa się za podwarstwy niezależnych bramek oraz podwarstwy splątania.\n",
        "\n",
        "\n",
        "Indeksy tych parametrów są numerowane od zera, a konstrukcja \" j*n_qubits * 4 \" upewnia się, że każda warstwa kożysta jedynie ze swoich parametrów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nLVltQH8rKQ"
      },
      "outputs": [],
      "source": [
        "def ansatz(n_qubits, depth):\n",
        "\n",
        "    #Stworzenie parametrów bramek rotacyjnych\n",
        "    theta = ParameterVector('θ', 8*depth)\n",
        "\n",
        "    #Inicjalizacja układu\n",
        "    qc = QuantumCircuit(n_qubits)\n",
        "\n",
        "    #Pętla warst\n",
        "    for j in range(depth//2):\n",
        "\n",
        "        #Warstwa 1/3(0-7)(16-23)\n",
        "        for i in range(n_qubits):\n",
        "            qc.ry(theta[j*n_qubits*4+i], i)\n",
        "        #Bariera upewnia się, że obliczeina zostaną wykonane liniowo, a nie skompresowane\n",
        "        qc.barrier()\n",
        "\n",
        "        qc.crx(theta[j*n_qubits*4+4], 3, 0)\n",
        "        qc.crx(theta[j*n_qubits*4+5], 2, 3)\n",
        "        qc.crx(theta[j*n_qubits*4+6], 1, 2)\n",
        "        qc.crx(theta[j*n_qubits*4+7], 0, 1)\n",
        "        qc.barrier()\n",
        "\n",
        "        #Warstwa 2/4(8-15)(25-31)\n",
        "        for i in range(n_qubits):\n",
        "            qc.rx(theta[j*n_qubits*4+8 + i], i)\n",
        "        qc.barrier()\n",
        "\n",
        "        qc.cry(theta[j*n_qubits*4+12], 3, 2)\n",
        "        qc.cry(theta[j*n_qubits*4+13], 0, 3)\n",
        "        qc.cry(theta[j*n_qubits*4+14], 1, 0)\n",
        "        qc.cry(theta[j*n_qubits*4+15], 2, 1)\n",
        "        qc.barrier()\n",
        "\n",
        "    return qc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dbdb644"
      },
      "outputs": [],
      "source": [
        "\n",
        "class HybridModel:\n",
        "    def __init__(self, ansatz_circuit, num_qubits):\n",
        "\n",
        "        # we inicialize the variables\n",
        "        self.num_qubits = num_qubits\n",
        "\n",
        "        '''now we perform data encoding using ZZFetureMap, briefly it consist of three layers\n",
        "        1. hadamard - puts all the qubits in superposition state |+>\n",
        "        2. Z-Rotations - rotates each qubit individually based on input features Rz(x)\n",
        "        3. ZZ-Rotations - it entangles qubits by rotating them based on the relationship between two features\n",
        "\n",
        "        reps=1, we do the circuit of encoding only once. We can try and change that to 2 but it gets deeper\n",
        "        '''\n",
        "        self.feature_map = ZZFeatureMap(feature_dimension=num_qubits, reps=1)\n",
        "\n",
        "        # Initalizing quantum circuit. Here we are connecting our feature map (data) and ansatz\n",
        "        self.qc = QuantumCircuit(num_qubits)\n",
        "        self.qc.compose(self.feature_map,qubits=range(num_qubits), inplace=True)\n",
        "        self.qc.compose(ansatz_circuit, inplace=True)\n",
        "\n",
        "\n",
        "        '''\n",
        "        So that is a crucial step. Firstly, we inicialize parameters. Our quantum model cannot tell whether the number came from ansatz or feature.\n",
        "        That is why here we sort them into two lists. If the number came from feature_map, then it will be a feature and the other way around.\n",
        "\n",
        "        '''\n",
        "        final_circuit_params = self.qc.parameters\n",
        "\n",
        "        # creating dicts with names of the variables to then check them in our loop\n",
        "        feature_map_names = {p.name for p in self.feature_map.parameters}\n",
        "        ansatz_names = {p.name for p in ansatz_circuit.parameters}\n",
        "\n",
        "        self.final_input_params = []\n",
        "        self.final_weight_params = []\n",
        "\n",
        "        for p in final_circuit_params:\n",
        "            if p.name in feature_map_names:\n",
        "                self.final_input_params.append(p)\n",
        "            elif p.name in ansatz_names:\n",
        "                self.final_weight_params.append(p)\n",
        "\n",
        "        observable = SparsePauliOp.from_list([(\"I\" * (num_qubits - 1) + \"Z\", 1)])\n",
        "\n",
        "        '''Estimator takes ansatz, observables and parameters (data and weights), returns the Expectation value.'''\n",
        "\n",
        "\n",
        "        # estimator = StatevectorEstimator()\n",
        "        estimator = Estimator()\n",
        "        # creating a gradient for backward\n",
        "        gradient = ParamShiftEstimatorGradient(estimator)\n",
        "        self.qnn = EstimatorQNN(\n",
        "            circuit=self.qc,\n",
        "            observables=observable,\n",
        "            input_params=self.final_input_params,\n",
        "            weight_params=self.final_weight_params,\n",
        "            estimator=estimator,\n",
        "            gradient = gradient\n",
        "        )\n",
        "\n",
        "\n",
        "    # forward: gives the Estimator data and weights, sets the qubit angles, runs gates and measures the result\n",
        "    def forward(self, x, weights):\n",
        "        return self.qnn.forward(x, weights)\n",
        "\n",
        "    # backward In Quantum ML it actually runs forward once again but with a little change\n",
        "    # Firstly it shifts the angles by pi/2 and runs the circuit, then -pi/2 and runs the circuit\n",
        "    # It calculates the difference and it's the gradient, we only return weight_grads because we dont need input_grads\n",
        "    def backward(self, x, weights):\n",
        "        _, weight_grads = self.qnn.backward(x, weights)\n",
        "        if weight_grads is None:\n",
        "            # If it fails, return zeros to prevent the loop from crashing\n",
        "            print(\"Warning: Gradients were None. Returning Zeros.\")\n",
        "            return np.zeros((x.shape[0], len(weights)))\n",
        "        return weight_grads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdIU8v5o1rxb"
      },
      "outputs": [],
      "source": [
        "my_ansatz = ansatz_A1(4, 4)\n",
        "qnn = HybridModel(\n",
        "    ansatz_circuit=my_ansatz,\n",
        "    num_qubits=5,\n",
        ")\n",
        "\n",
        "\n",
        "# Inicializing the weights\n",
        "num_weights = qnn.qnn.num_weights\n",
        "rng = np.random.default_rng(seed=42)\n",
        "weights = 2 * np.pi * rng.random(num_weights)\n",
        "weights = weights.flatten()\n",
        "\n",
        "print(f\"Weights initialized. Shape: {weights.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3DN1Zs6qEiu"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.02\n",
        "\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "acc_history = []\n",
        "\n",
        "print(\"Pobieranie danych z funkcji prepare_data()...\")\n",
        "\n",
        "# Zwraca ona już przeskalowane cechy \"X\" w zakresie [0, pi], więc z X nie musimy juz nic robic.\n",
        "X_train, X_test, y_train_raw, y_test_raw = prepare_data()\n",
        "\n",
        "# Funkcja prepare_data zwraca etykiety {0, 1}.\n",
        "# Nasz model HybridModel używa pomiaru Z (wartości od -1 do 1).\n",
        "# Konwertujemy: 0 -> -1 oraz 1 -> 1\n",
        "y_train = 2 * y_train_raw - 1\n",
        "y_test = 2 * y_test_raw - 1\n",
        "\n",
        "# Upewniamy się, że typy danych to float32 (najlepsze dla PyTorch/NumPy)\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "print(f\"Dane gotowe. Liczba próbek treningowych: {len(X_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE4cX2_kqEiv"
      },
      "outputs": [],
      "source": [
        "# PRZYGOTOWANIE OPTYMALIZATORA ADAM\n",
        "# Tworzymy Tensor PyTorcha, który będzie przechowywał wagi i historię gradientów\n",
        "weights_tensor = torch.tensor(weights, requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "# Używamy adam optimizer\n",
        "optimizer = torch.optim.Adam([weights_tensor], lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"Start treningu... Model: HybridModel, Epoki: {EPOCHS}, LR: {LEARNING_RATE}\")# GŁÓWNA PĘTLA TRENINGOWA\n",
        "for epoch in range(EPOCHS):\n",
        "    # Tasowanie danych w każdej epoce\n",
        "    X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train, random_state=epoch)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    batches_count = 0\n",
        "\n",
        "    # Iteracja po batchach\n",
        "    for i in range(0, len(X_train), BATCH_SIZE):\n",
        "        X_batch = X_train_shuffled[i:i + BATCH_SIZE]\n",
        "        y_batch = y_train_shuffled[i:i + BATCH_SIZE]\n",
        "\n",
        "        # Konwersja wag na NumPy dla Qiskita (bo Qiskit nie przyjmuje tensorów PyTorcha)\n",
        "        current_weights_numpy = weights_tensor.detach().numpy()\n",
        "\n",
        "        # forward\n",
        "        # Predykcja modelu (zwraca wartości od -1 do 1)\n",
        "        pred = qnn.forward(X_batch, current_weights_numpy)\n",
        "\n",
        "        # backward\n",
        "        # Obliczenie gradientów wag (zwraca numpy array)\n",
        "        grads_numpy = qnn.backward(X_batch, current_weights_numpy)\n",
        "\n",
        "        # MSE\n",
        "        # Różnica: predykcja - prawda\n",
        "        # Reshape tylko dla y_batch, bo pred wychodząc z QNN jest juz w kształcie (32,1)\n",
        "        # Reshape od flatten rozni się tym ze flatten spłaszcza wszystko do jednego wymiaru i towrzy prostą listę liczb np. [1,2,3]\n",
        "        # reshape(-1,1) natomiast układa te same liczby w pionową kolumnę np. [[1], [2], [3]], reshape(3,1)- 3 wiersze i 1 kolumna\n",
        "        # reshape(-1,1) -1 oznacza ze bedzie 1 kolumna i parametr -1 zamienia sie automatycznie w BATCH_SIZE czyli 32\n",
        "        diff = pred - y_batch.reshape(-1, 1)\n",
        "        loss = np.mean(diff ** 2)\n",
        "\n",
        "        # Chain Rule: pochodna MSE = 2 * (pred - y)\n",
        "        # 2 * diff bo to pochodna z (pred - y)^2\n",
        "        grad_modifier = 2 * diff\n",
        "\n",
        "        # OBLICZANIE GRADIENTU\n",
        "        # Usuwamy zbędny środkowy wymiar (z (32, 1, wagi) na (32, wagi))\n",
        "        # Dzięki temu mamy prostą macierz: wiersz = próbka, kolumna = gradient wagi\n",
        "        grads_2d = grads_numpy.reshape(grads_numpy.shape[0], -1)\n",
        "\n",
        "        # Ważymy gradienty (Chain Rule)\n",
        "        # Każdy wiersz gradientów mnożymy przez błąd danej próbki (grad_modifier)\n",
        "        weighted_grads = grad_modifier * grads_2d\n",
        "\n",
        "        # Obliczamy średnią po całym batchu (axis=0 to wiersze)\n",
        "        # To daje nam jeden wektor gradientów do aktualizacji wag\n",
        "        batch_grads_numpy = np.mean(weighted_grads, axis=0)\n",
        "\n",
        "        # AKTUALIZACJA WAG\n",
        "        # Czyścimy stare gradienty\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Dajemy nasz ręcznie policzony gradient do tensora PyTorcha\n",
        "        weights_tensor.grad = torch.from_numpy(batch_grads_numpy)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss\n",
        "        batches_count += 1\n",
        "\n",
        "    # Zapisanie wytrenowanych wag z powrotem do zmiennej numpy\n",
        "    weights = weights_tensor.detach().numpy()\n",
        "\n",
        "    test_outputs = qnn.forward(X_test, weights)\n",
        "    test_diff = test_outputs - y_test.reshape(-1, 1)\n",
        "    test_loss = np.mean(test_diff ** 2)\n",
        "\n",
        "    # zamiast predicted = (test_outputs > 0.5).astype(int).flatten()\n",
        "    # bo nasze etykiety y_test to [-1, 1] a nie [0, 1]\n",
        "    # Używamy np.where: jeśli wynik > 0 to klasa 1, w przeciwnym razie klasa -1.\n",
        "    predicted = np.where(test_outputs > 0, 1, -1).flatten()\n",
        "\n",
        "    test_accuracy = np.mean(predicted == y_test.flatten())\n",
        "\n",
        "    avg_loss = epoch_loss / batches_count\n",
        "    train_loss_history.append(avg_loss)\n",
        "    test_loss_history.append(test_loss)\n",
        "    acc_history.append(test_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Avg loss: {avg_loss:.4f} | Test Acc: {test_accuracy:.4f}\")\n",
        "    print(\"Trening zakończony.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VXadi_IqEiw"
      },
      "outputs": [],
      "source": [
        "test_outputs = qnn.forward(X_test, weights)\n",
        "predicted = np.where(test_outputs > 0, 1, -1).flatten() # Tutaj tez uzywamy np.where\n",
        "\n",
        "print(confusion_matrix(y_test, predicted))\n",
        "print(f1_score(y_test, predicted))\n",
        "\n",
        "epochs = range(1, EPOCHS + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss_history, label='Train Loss', color='blue')\n",
        "plt.plot(epochs, test_loss_history, label='Test Loss', color='red', linestyle='--')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, acc_history, label='Test Accuracy', color='green')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
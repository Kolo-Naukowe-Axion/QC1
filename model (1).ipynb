{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantum Classfier"
      ],
      "metadata": {
        "id": "8jLzI6vhj-pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.02\n",
        "\n",
        "weights = initial_weights.copy()\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "acc_history = []\n",
        "\n",
        "optimizer = AdamOptimizer(weights.shape, lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train, random_state=epoch)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    batches_count = 0\n",
        "\n",
        "    for i in range(0, len(X_train), BATCH_SIZE):\n",
        "        X_batch = X_train_shuffled[i:i + BATCH_SIZE]\n",
        "        y_batch = y_train_shuffled[i:i + BATCH_SIZE]\n",
        "\n",
        "        pred = qnn.forward(X_batch, weights)\n",
        "        _, grads = qnn.backward(X_batch, weights)\n",
        "\n",
        "        diff = pred - y_batch.reshape(-1, 1)\n",
        "        loss = np.mean(diff ** 2)\n",
        "\n",
        "        grad_modifier = 2 * diff\n",
        "\n",
        "        batch_grads = np.mean(grad_modifier[:, :, np.newaxis] * grads, axis=0).squeeze()\n",
        "\n",
        "        weights = optimizer.step(weights, batch_grads)\n",
        "\n",
        "        epoch_loss += loss\n",
        "        batches_count += 1\n",
        "\n",
        "    test_outputs = qnn.forward(X_test, weights)\n",
        "\n",
        "    test_diff = test_outputs - y_test.reshape(-1, 1)\n",
        "    test_loss = np.mean(test_diff ** 2)\n",
        "\n",
        "    predicted = (test_outputs > 0.5).astype(int).flatten()\n",
        "    test_accuracy = np.mean(predicted == y_test.flatten())\n",
        "\n",
        "    avg_loss = epoch_loss / batches_count\n",
        "    train_loss_history.append(avg_loss)\n",
        "    test_loss_history.append(test_loss)\n",
        "    acc_history.append(test_accuracy)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Avg loss: {avg_loss:.4f} | Test Acc: {test_accuracy:.4f}\")\n",
        "\n",
        "test_outputs = qnn.forward(X_test, weights)\n",
        "predicted = (test_outputs > 0.5).astype(int).flatten()\n",
        "print(confusion_matrix(y_test, predicted))\n",
        "print(f1_score(y_test, predicted))"
      ],
      "metadata": {
        "id": "CcmFbWrbVe6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, EPOCHS + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss_history, label='Train Loss', color='blue')\n",
        "plt.plot(epochs, test_loss_history, label='Test Loss', color='red', linestyle='--')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, acc_history, label='Test Accuracy', color='green')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mzRJ43gykCr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
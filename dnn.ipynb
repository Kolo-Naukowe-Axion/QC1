{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kolo-Naukowe-Axion/QC1/blob/dnn/dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Banknote Authentication DNN Developed a binary classification model using PyTorch to detect counterfeit banknotes based on four mathematical image parameters (Variance, Skewness, Kurtosis, Entropy). The solution is based on the UCI Banknote Authentication Data Set, containing 1372 records. Prior Exploratory Data Analysis (EDA) revealed that the dataset is well-balanced (55% authentic / 45% counterfeit) and confirmed a clear separation between authentic and counterfeit classes. This strong linear separability allowed the neural network, combined with StandardScaler normalization, to easily define decision boundaries and achieve near-perfect accuracy on the test set. For benchmarking purposes, a Logistic Regression model was also trained, reaching ~98% accuracy, which further corroborates the high quality and distinct separation of the dataset features."
      ],
      "metadata": {
        "id": "27Qwfv9FpJK7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pcXZafusqqK2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60555a53-3695-42b7-d558-5a899f7fcbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2026.1.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "# taking data from the UCI Machine Learning repository\n",
        "!pip install ucimlrepo\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "gM7Icsn7V4a9",
        "outputId": "158706ea-d6bd-493a-b561-5b9dc4d28bb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking data, conversion to numpy"
      ],
      "metadata": {
        "id": "zW-SX5adq2Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "banknote_authentication = fetch_ucirepo(id=267)\n",
        "\n",
        "X_raw = banknote_authentication.data.features\n",
        "y_raw = banknote_authentication.data.targets\n",
        "\n",
        "X = X_raw.values\n",
        "y = y_raw.values.ravel()"
      ],
      "metadata": {
        "id": "vNmCRe1Cq_Q-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test and train split, 80% train and 20% test"
      ],
      "metadata": {
        "id": "0UFJ8FkrrkIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# stadarization\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "print(f\"Training Tensor Shape: {X_train_tensor.shape}\") # 4 different parameters for one banknote: variance\tskewness\tkurtosis\tentropy\n",
        "print(f\"Testing Tensor Shape: {X_test_tensor.shape}\")"
      ],
      "metadata": {
        "id": "vEkkupNXrmq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412646fb-3571-4cd0-e874-4c95e7760394"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Tensor Shape: torch.Size([1097, 4])\n",
            "Testing Tensor Shape: torch.Size([275, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# building neural network feed-forward with 3 hidden layers\n",
        "\n",
        "class BanknoteDNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BanknoteDNN, self).__init__()\n",
        "        self.layer1 = nn.Linear(4, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 16)\n",
        "        self.output = nn.Linear(16, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x = self.sigmoid(self.output(x)) # sigmoid activation, as we want the probability (0 - fake money, 1 - real money)\n",
        "        return x\n",
        "\n",
        "model = BanknoteDNN().to(device)"
      ],
      "metadata": {
        "id": "KqCrWK8EseAb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 100\n",
        "print(\"\\nStarting training...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    outputs = model(X_train_tensor)\n",
        "\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(X_test_tensor)\n",
        "        test_loss = criterion(test_outputs, y_test_tensor)\n",
        "        predicted = (test_outputs > 0.5).float()\n",
        "        correct = (predicted == y_test_tensor).float().sum()\n",
        "        accuracy = correct / y_test_tensor.shape[0]\n",
        "\n",
        "    if epoch%10==0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {loss.item():.4f} | Test Loss: {test_loss.item():.4f} | Test Acc: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7wh4QxGstoz",
        "outputId": "324a1996-c729-4bd8-ea37-b040252bea67"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "Epoch 1/100 | Train Loss: 0.7131 | Test Loss: 0.7050 | Test Acc: 0.4618\n",
            "Epoch 11/100 | Train Loss: 0.6882 | Test Loss: 0.6832 | Test Acc: 0.4618\n",
            "Epoch 21/100 | Train Loss: 0.6539 | Test Loss: 0.6511 | Test Acc: 0.8145\n",
            "Epoch 31/100 | Train Loss: 0.5962 | Test Loss: 0.5948 | Test Acc: 0.8545\n",
            "Epoch 41/100 | Train Loss: 0.5128 | Test Loss: 0.5125 | Test Acc: 0.8545\n",
            "Epoch 51/100 | Train Loss: 0.4015 | Test Loss: 0.4037 | Test Acc: 0.8800\n",
            "Epoch 61/100 | Train Loss: 0.2808 | Test Loss: 0.2873 | Test Acc: 0.9055\n",
            "Epoch 71/100 | Train Loss: 0.1840 | Test Loss: 0.1851 | Test Acc: 0.9382\n",
            "Epoch 81/100 | Train Loss: 0.1147 | Test Loss: 0.1136 | Test Acc: 0.9745\n",
            "Epoch 91/100 | Train Loss: 0.0824 | Test Loss: 0.0713 | Test Acc: 0.9818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "preds = clf.predict(X_test)\n",
        "print(f\"Simple model accuracy: {accuracy_score(y_test, preds)*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9D9CkQPN3ee",
        "outputId": "543e85b8-e6d3-4d17-aeae-8f5cb21d6812"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple model accuracy: 97.82%\n"
          ]
        }
      ]
    }
  ]
}